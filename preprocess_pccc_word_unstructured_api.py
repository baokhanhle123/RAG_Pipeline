#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Tiá»n xá»­ lÃ½ Word (.doc/.docx) vÄƒn báº£n luáº­t PCCC báº±ng Unstructured API.
- Gá»­i file qua multipart/form-data
- Nháº­n list element (Title/NarrativeText/ListItem/Table/... + metadata)
- Gáº¯n thÃªm metadata phÃ¡p lÃ½ (chuong/muc/dieu/khoan/diem/phu_luc) Ä‘á»ƒ phá»¥c vá»¥ trÃ­ch dáº«n
- Xuáº¥t JSONL (1 dÃ²ng / element) cho pipeline RAG

YÃªu cáº§u:
  pip install requests python-dateutil
Biáº¿n mÃ´i trÆ°á»ng:
  UNSTRUCTURED_API_KEY: API key
    : máº·c Ä‘á»‹nh dÃ¹ng free endpoint https://api.unstructured.io/general/v0/general
Tham kháº£o API & tham sá»‘: docs.unstructured.io (partition endpoint, api parameters)

export UNSTRUCTURED_API_URL="https://api.unstructuredapp.io/general/v0/general"
export UNSTRUCTURED_API_KEY=""
"""

from __future__ import annotations
import os, re, json, time, uuid, hashlib, mimetypes
from pathlib import Path
from typing import Dict, List, Any, Iterable, Optional, Tuple
import requests

# =========================
# Cáº¥u hÃ¬nh & tiá»‡n Ã­ch chung
# =========================

API_URL = os.getenv("UNSTRUCTURED_API_URL", "").strip()
API_KEY = os.getenv("UNSTRUCTURED_API_KEY", "").strip()

# MIME cho Word
MIME_BY_EXT = {
    ".docx": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
    ".doc":  "application/msword",
}

HEADERS = {
    "accept": "application/json",
    # CHUáº¨N ÄÃšNG: KhÃ´ng dÃ¹ng Authorization Bearer
    "unstructured-api-key": API_KEY if API_KEY else "",
}

def sha1_of_file(p: Path, chunk: int = 1 << 20) -> str:
    h = hashlib.sha1()
    with p.open("rb") as f:
        for b in iter(lambda: f.read(chunk), b""):
            h.update(b)
    return h.hexdigest()

def guess_mime(path: Path) -> str:
    return MIME_BY_EXT.get(path.suffix.lower(), mimetypes.guess_type(str(path))[0] or "application/octet-stream")

# ================
# Gá»i Unstructured
# ================

def partition_word_via_unstructured_api(
    file_path: Path,
    languages: List[str] = ["vie", "eng"],
    include_page_breaks: bool = True,
    coordinates: bool = True,
    encoding: str = "utf-8",
    timeout: int = 120,
    max_retries: int = 3,
    backoff_sec: float = 2.0,
) -> List[Dict[str, Any]]:
    assert API_KEY, (
        "Thiáº¿u UNSTRUCTURED_API_KEY. HÃ£y `export UNSTRUCTURED_API_KEY=...` "
        "vÃ  Ä‘áº£m báº£o header 'unstructured-api-key' Ä‘Æ°á»£c set."
    )
    assert API_URL.startswith("http"), (
        "Thiáº¿u/khÃ´ng há»£p lá»‡ UNSTRUCTURED_API_URL. VÃ­ dá»¥ Free: "
        "https://api.unstructured.io/general/v0/general; "
        "Starter/Team: URL hiá»ƒn thá»‹ trong tÃ i khoáº£n."
    )
    assert file_path.exists(), f"KhÃ´ng tháº¥y file: {file_path}"

    data = {
        "languages": json.dumps(languages),          # multipart: truyá»n list dÆ°á»›i dáº¡ng JSON string
        "include_page_breaks": str(include_page_breaks).lower(),
        "coordinates": str(coordinates).lower(),
        "encoding": encoding,
        "output_format": "application/json",
    }

    last_err = None
    for attempt in range(1, max_retries + 1):
        try:
            mime = guess_mime(file_path)
            # QUAN TRá»ŒNG: Má»Ÿ láº¡i file má»—i láº§n retry Ä‘á»ƒ khÃ´ng bá»‹ EOF
            with file_path.open("rb") as f:
                files = {"files": (file_path.name, f, mime)}
                resp = requests.post(API_URL, headers=HEADERS, data=data, files=files, timeout=timeout)

            if resp.status_code == 200:
                out = resp.json()
                if isinstance(out, dict) and "elements" in out:
                    return out["elements"] or []
                if isinstance(out, list):
                    return out
                return out if isinstance(out, list) else []

            # Gá»£i Ã½ cháº©n Ä‘oÃ¡n thÃ´ng minh cho 401/403
            if resp.status_code in (401, 403):
                msg = resp.text
                hint = []
                if "API key is missing" in msg or "missing" in msg.lower():
                    hint.append("Header pháº£i lÃ  'unstructured-api-key', khÃ´ng pháº£i 'Authorization'.")
                hint.append("Kiá»ƒm tra cáº·p URLâ€“Key: Free dÃ¹ng api.unstructured.io; Starter/Team dÃ¹ng URL .app.io (xem account).")
                raise RuntimeError(f"Auth lá»—i {resp.status_code}: {msg[:300]} | Gá»£i Ã½: " + " ".join(hint))

            if resp.status_code in (429, 500, 502, 503, 504):
                last_err = RuntimeError(f"{resp.status_code} {resp.text[:300]}")
                time.sleep(backoff_sec * attempt)
                continue

            raise RuntimeError(f"API lá»—i {resp.status_code}: {resp.text[:1000]}")

        except requests.RequestException as e:
            last_err = e
            time.sleep(backoff_sec * attempt)
            continue

    raise last_err or RuntimeError("Partition failed without explicit error.")

# ===========================
# Chuáº©n hoÃ¡ & trÃ­ch xuáº¥t luáº­t
# ===========================

_re_chuong  = re.compile(r"^\s*ChÆ°Æ¡ng\s+([IVXLCDM]+)\b", re.IGNORECASE)
_re_muc     = re.compile(r"^\s*Má»¥c\s+([IVXLCDM]+)\b", re.IGNORECASE)
_re_dieu    = re.compile(r"^\s*Äiá»u\s+(\d+[A-Za-z]*)\b", re.IGNORECASE)
_re_khoan   = re.compile(r"^\s*Khoáº£n\s+(\d+)\b", re.IGNORECASE)
_re_diem    = re.compile(r"^\s*([a-zA-Z])\)", re.IGNORECASE)  # "a) b) c)"
_re_phuluc  = re.compile(r"^\s*Phá»¥\s*lá»¥c\s*([A-Z0-9\-]+)?\b", re.IGNORECASE)

# Nháº­n diá»‡n loáº¡i vÄƒn báº£n & sá»‘ hiá»‡u (cÆ¡ báº£n, cÃ³ thá»ƒ má»Ÿ rá»™ng)
_re_loai_vb = re.compile(
    r"\b(?:Luáº­t|Nghá»‹\s*Ä‘á»‹nh|ThÃ´ng\s*tÆ°|QCVN|TCVN)\b[^\n]*", re.IGNORECASE
)

def enrich_hierarchy(elements: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Duyá»‡t tuáº§n tá»± element Ä‘á»ƒ gáº¯n tráº¡ng thÃ¡i ChÆ°Æ¡ng/Má»¥c/Äiá»u/Khoáº£n/Äiá»ƒm/Phá»¥ lá»¥c.
    Äá»“ng thá»i suy Ä‘oÃ¡n 'van_ban' (tÃªn vÄƒn báº£n/sá»‘ hiá»‡u) tá»« cÃ¡c Title/Heading/NarrativeText Ä‘áº§u tÃ i liá»‡u.
    """
    chuong = muc = dieu = khoan = diem = phu_luc = None
    van_ban: Optional[str] = None

    # Thá»­ suy Ä‘oÃ¡n vÄƒn báº£n á»Ÿ 50 element Ä‘áº§u
    head_text = []
    for el in elements[:50]:
        t = (el.get("text") or "").strip()
        if t:
            head_text.append(t)
    joined = "\n".join(head_text)
    m = _re_loai_vb.search(joined)
    if m:
        van_ban = m.group(0).strip()

    out: List[Dict[str, Any]] = []
    for el in elements:
        text = (el.get("text") or "").strip()
        if not text:
            # váº«n giá»¯ láº¡i (vÃ­ dá»¥ PageBreak) Ä‘á»ƒ báº£o toÃ n sá»‘ trang
            pass

        # cáº­p nháº­t má»©c tiÃªu Ä‘á» náº¿u khá»›p
        if _re_chuong.match(text):
            chuong, muc, dieu, khoan, diem = _re_chuong.match(text).group(1), None, None, None, None
        elif _re_muc.match(text):
            muc, dieu, khoan, diem = _re_muc.match(text).group(1), None, None, None
        elif _re_dieu.match(text):
            dieu, khoan, diem = _re_dieu.match(text).group(1), None, None
        elif _re_khoan.match(text):
            khoan, diem = _re_khoan.match(text).group(1), None
        elif _re_diem.match(text):
            diem = _re_diem.match(text).group(1)
        elif _re_phuluc.match(text):
            phu_luc = _re_phuluc.match(text).group(1) or ""

        meta = el.get("metadata") or {}
        # gáº¯n hierarchy vÃ o metadata má»›i
        meta_enriched = {
            **meta,
            "chuong": chuong,
            "muc": muc,
            "dieu": dieu,
            "khoan": khoan,
            "diem": diem,
            "phu_luc": phu_luc,
            "van_ban": van_ban,
        }

        # táº¡o báº£n ghi chuáº©n hoÃ¡
        normalized = {
            "doc_id": meta.get("filename") or "",
            "source_sha1": meta.get("sha256") or "",  # API Ä‘Ã´i khi cung cáº¥p sha256; náº¿u khÃ´ng cÃ³ sáº½ gÃ¡n á»Ÿ ngoÃ i
            "element_id": el.get("id") or str(uuid.uuid4()),
            "type": el.get("type"),
            "text": text,
            "metadata": meta_enriched,
        }
        out.append(normalized)
    return out

def attach_file_level_metadata(
    normalized: List[Dict[str, Any]],
    file_path: Path,
    source_sha1: str,
) -> List[Dict[str, Any]]:
    for row in normalized:
        row["doc_id"] = file_path.name
        if not row.get("source_sha1"):
            row["source_sha1"] = source_sha1
        # Ä‘áº£m báº£o cÃ³ page_number náº¿u API Ä‘Ã£ chÃ¨n PageBreak
        # Unstructured sáº½ táº¡o element type="PageBreak" vá»›i metadata.page_number tÄƒng dáº§n náº¿u báº­t include_page_breaks
        # (kháº£ dá»¥ng cho má»™t sá»‘ Ä‘á»‹nh dáº¡ng; vá»›i .docx cÃ³ thá»ƒ khÃ´ng luÃ´n cÃ³ trang)
    return normalized

def write_jsonl(records: Iterable[Dict[str, Any]], out_path: Path) -> int:
    out_path.parent.mkdir(parents=True, exist_ok=True)
    n = 0
    with out_path.open("w", encoding="utf-8") as f:
        for rec in records:
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")
            n += 1
    return n

# =====================
# Pipeline xá»­ lÃ½ thÆ° má»¥c
# =====================

def preprocess_word_folder(
    inputs: List[Path],
    out_jsonl: Path,
    languages: List[str] = ["vie","eng"],
) -> int:
    """
    Xá»­ lÃ½ danh sÃ¡ch file .doc/.docx â†’ out_jsonl (append).
    """
    total = 0
    out_jsonl.parent.mkdir(parents=True, exist_ok=True)

    for p in inputs:
        if not p.exists():
            print(f"âš ï¸  Bá» qua (khÃ´ng tá»“n táº¡i): {p}")
            continue
        if p.suffix.lower() not in (".doc", ".docx"):
            print(f"âš ï¸  Bá» qua (khÃ´ng pháº£i Word): {p.name}")
            continue

        print(f"ğŸ”¹ Partition: {p.name}")
        elements = partition_word_via_unstructured_api(
            p,
            languages=languages,
            include_page_breaks=True,
            coordinates=True,
            encoding="utf-8",
        )
        source_sha1 = sha1_of_file(p)
        normalized = enrich_hierarchy(elements)
        normalized = attach_file_level_metadata(normalized, p, source_sha1)

        with out_jsonl.open("a", encoding="utf-8") as f:
            for rec in normalized:
                f.write(json.dumps(rec, ensure_ascii=False) + "\n")
                total += 1

    print(f"âœ… ÄÃ£ ghi {total} element â†’ {out_jsonl}")
    return total

# ============
# VÃ­ dá»¥ cháº¡y thá»­
# ============

if __name__ == "__main__":
    # Thay Ä‘á»•i Ä‘Æ°á»ng dáº«n theo mÃ´i trÆ°á»ng cá»§a báº¡n
    # (CÃ¡c file máº«u user Ä‘Ã£ upload á»Ÿ /mnt/data)
    sample_paths = [
        Path("dataset/LuaÌ£Ì‚t PCCC 2024 55_2024_QH15_621347.doc"),
        Path("dataset/NghiÌ£ Ä‘iÌ£nh 105-2025-NÄ-CP ngaÌ€y 15-05-2025 huÌ›oÌ›Ìng daÌ‚Ìƒn LuaÌ£Ì‚t PhoÌ€ng chaÌy, chuÌ›Ìƒa chaÌy vaÌ€ cuÌ›Ìu naÌ£n, cuÌ›Ìu hoÌ£Ì‚.doc"),
        Path("dataset/QCVN 03 2023 BCA veÌ‚Ì€ PhuÌ›oÌ›ng tieÌ£Ì‚n phoÌ€ng chaÌy vaÌ€ chuÌ›Ìƒa chaÌy_VN.doc"),
        Path("dataset/B.1. BaÌ‰ng Ä‘oÌ‚Ìi chieÌ‚Ìu quy hoaÌ£ch.docx"),
    ]
    out = Path("./pccc_word_elements.jsonl")
    preprocess_word_folder(sample_paths, out)
