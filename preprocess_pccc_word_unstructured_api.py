#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Ti·ªÅn x·ª≠ l√Ω Word (.doc/.docx) vƒÉn b·∫£n lu·∫≠t PCCC b·∫±ng Unstructured API.
- G·ª≠i file qua multipart/form-data
- Nh·∫≠n list element (Title/NarrativeText/ListItem/Table/... + metadata)
- G·∫Øn th√™m metadata ph√°p l√Ω (chuong/muc/dieu/khoan/diem/phu_luc) ƒë·ªÉ ph·ª•c v·ª• tr√≠ch d·∫´n
- Xu·∫•t JSONL (1 d√≤ng / element) cho pipeline RAG

Y√™u c·∫ßu:
  pip install requests python-dateutil
Bi·∫øn m√¥i tr∆∞·ªùng:
  UNSTRUCTURED_API_KEY: API key
    : m·∫∑c ƒë·ªãnh d√πng free endpoint https://api.unstructured.io/general/v0/general
Tham kh·∫£o API & tham s·ªë: docs.unstructured.io (partition endpoint, api parameters)

export UNSTRUCTURED_API_URL="https://api.unstructuredapp.io/general/v0/general"
export UNSTRUCTURED_API_KEY=""
"""

from __future__ import annotations
import os, re, json, time, uuid, hashlib, mimetypes
from pathlib import Path
from typing import Dict, List, Any, Iterable, Optional, Tuple
import requests

# =========================
# C·∫•u h√¨nh & ti·ªán √≠ch chung
# =========================

API_URL = os.getenv("UNSTRUCTURED_API_URL", "").strip()
API_KEY = os.getenv("UNSTRUCTURED_API_KEY", "").strip()

# MIME cho Word
MIME_BY_EXT = {
    ".docx": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
    ".doc":  "application/msword",
}

HEADERS = {
    "accept": "application/json",
    # CHU·∫®N ƒê√öNG: Kh√¥ng d√πng Authorization Bearer
    "unstructured-api-key": API_KEY if API_KEY else "",
}

def sha1_of_file(p: Path, chunk: int = 1 << 20) -> str:
    h = hashlib.sha1()
    with p.open("rb") as f:
        for b in iter(lambda: f.read(chunk), b""):
            h.update(b)
    return h.hexdigest()

def guess_mime(path: Path) -> str:
    return MIME_BY_EXT.get(path.suffix.lower(), mimetypes.guess_type(str(path))[0] or "application/octet-stream")

# ================
# G·ªçi Unstructured
# ================

def partition_word_via_unstructured_api(
    file_path: Path,
    languages: List[str] = ["vie", "eng"],
    include_page_breaks: bool = True,
    coordinates: bool = True,
    encoding: str = "utf-8",
    timeout: int = 120,
    max_retries: int = 3,
    backoff_sec: float = 2.0,
) -> List[Dict[str, Any]]:
    assert API_KEY, (
        "Thi·∫øu UNSTRUCTURED_API_KEY. H√£y `export UNSTRUCTURED_API_KEY=...` "
        "v√† ƒë·∫£m b·∫£o header 'unstructured-api-key' ƒë∆∞·ª£c set."
    )
    assert API_URL.startswith("http"), (
        "Thi·∫øu/kh√¥ng h·ª£p l·ªá UNSTRUCTURED_API_URL. V√≠ d·ª• Free: "
        "https://api.unstructured.io/general/v0/general; "
        "Starter/Team: URL hi·ªÉn th·ªã trong t√†i kho·∫£n."
    )
    assert file_path.exists(), f"Kh√¥ng th·∫•y file: {file_path}"

    data = {
        "languages": json.dumps(languages),          # multipart: truy·ªÅn list d∆∞·ªõi d·∫°ng JSON string
        "include_page_breaks": str(include_page_breaks).lower(),
        "coordinates": str(coordinates).lower(),
        "encoding": encoding,
        "output_format": "application/json",
    }

    last_err = None
    for attempt in range(1, max_retries + 1):
        try:
            mime = guess_mime(file_path)
            # QUAN TR·ªåNG: M·ªü l·∫°i file m·ªói l·∫ßn retry ƒë·ªÉ kh√¥ng b·ªã EOF
            with file_path.open("rb") as f:
                files = {"files": (file_path.name, f, mime)}
                resp = requests.post(API_URL, headers=HEADERS, data=data, files=files, timeout=timeout)

            if resp.status_code == 200:
                out = resp.json()
                if isinstance(out, dict) and "elements" in out:
                    return out["elements"] or []
                if isinstance(out, list):
                    return out
                return out if isinstance(out, list) else []

            # G·ª£i √Ω ch·∫©n ƒëo√°n th√¥ng minh cho 401/403
            if resp.status_code in (401, 403):
                msg = resp.text
                hint = []
                if "API key is missing" in msg or "missing" in msg.lower():
                    hint.append("Header ph·∫£i l√† 'unstructured-api-key', kh√¥ng ph·∫£i 'Authorization'.")
                hint.append("Ki·ªÉm tra c·∫∑p URL‚ÄìKey: Free d√πng api.unstructured.io; Starter/Team d√πng URL .app.io (xem account).")
                raise RuntimeError(f"Auth l·ªói {resp.status_code}: {msg[:300]} | G·ª£i √Ω: " + " ".join(hint))

            if resp.status_code in (429, 500, 502, 503, 504):
                last_err = RuntimeError(f"{resp.status_code} {resp.text[:300]}")
                time.sleep(backoff_sec * attempt)
                continue

            raise RuntimeError(f"API l·ªói {resp.status_code}: {resp.text[:1000]}")

        except requests.RequestException as e:
            last_err = e
            time.sleep(backoff_sec * attempt)
            continue

    raise last_err or RuntimeError("Partition failed without explicit error.")

# ===========================
# Chu·∫©n ho√° & tr√≠ch xu·∫•t lu·∫≠t
# ===========================

_re_chuong  = re.compile(r"^\s*Ch∆∞∆°ng\s+([IVXLCDM]+)\b", re.IGNORECASE)
_re_muc     = re.compile(r"^\s*M·ª•c\s+([IVXLCDM]+)\b", re.IGNORECASE)
_re_dieu    = re.compile(r"^\s*ƒêi·ªÅu\s+(\d+[A-Za-z]*)\b", re.IGNORECASE)
_re_khoan   = re.compile(r"^\s*Kho·∫£n\s+(\d+)\b", re.IGNORECASE)
_re_diem    = re.compile(r"^\s*([a-zA-Z])\)", re.IGNORECASE)  # "a) b) c)"
_re_phuluc  = re.compile(r"^\s*Ph·ª•\s*l·ª•c\s*([A-Z0-9\-]+)?\b", re.IGNORECASE)

# Nh·∫≠n di·ªán lo·∫°i vƒÉn b·∫£n & s·ªë hi·ªáu (c∆° b·∫£n, c√≥ th·ªÉ m·ªü r·ªông)
_re_loai_vb = re.compile(
    r"\b(?:Lu·∫≠t|Ngh·ªã\s*ƒë·ªãnh|Th√¥ng\s*t∆∞|QCVN|TCVN)\b[^\n]*", re.IGNORECASE
)

def enrich_hierarchy(elements: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Duy·ªát tu·∫ßn t·ª± element ƒë·ªÉ g·∫Øn tr·∫°ng th√°i Ch∆∞∆°ng/M·ª•c/ƒêi·ªÅu/Kho·∫£n/ƒêi·ªÉm/Ph·ª• l·ª•c.
    ƒê·ªìng th·ªùi suy ƒëo√°n 'van_ban' (t√™n vƒÉn b·∫£n/s·ªë hi·ªáu) t·ª´ c√°c Title/Heading/NarrativeText ƒë·∫ßu t√†i li·ªáu.
    """
    chuong = muc = dieu = khoan = diem = phu_luc = None
    van_ban: Optional[str] = None

    # Th·ª≠ suy ƒëo√°n vƒÉn b·∫£n ·ªü 50 element ƒë·∫ßu
    head_text = []
    for el in elements[:50]:
        t = (el.get("text") or "").strip()
        if t:
            head_text.append(t)
    joined = "\n".join(head_text)
    m = _re_loai_vb.search(joined)
    if m:
        van_ban = m.group(0).strip()

    out: List[Dict[str, Any]] = []
    for el in elements:
        text = (el.get("text") or "").strip()
        if not text:
            # v·∫´n gi·ªØ l·∫°i (v√≠ d·ª• PageBreak) ƒë·ªÉ b·∫£o to√†n s·ªë trang
            pass

        # c·∫≠p nh·∫≠t m·ª©c ti√™u ƒë·ªÅ n·∫øu kh·ªõp
        if _re_chuong.match(text):
            chuong, muc, dieu, khoan, diem = _re_chuong.match(text).group(1), None, None, None, None
        elif _re_muc.match(text):
            muc, dieu, khoan, diem = _re_muc.match(text).group(1), None, None, None
        elif _re_dieu.match(text):
            dieu, khoan, diem = _re_dieu.match(text).group(1), None, None
        elif _re_khoan.match(text):
            khoan, diem = _re_khoan.match(text).group(1), None
        elif _re_diem.match(text):
            diem = _re_diem.match(text).group(1)
        elif _re_phuluc.match(text):
            phu_luc = _re_phuluc.match(text).group(1) or ""

        meta = el.get("metadata") or {}
        # g·∫Øn hierarchy v√†o metadata m·ªõi
        meta_enriched = {
            **meta,
            "chuong": chuong,
            "muc": muc,
            "dieu": dieu,
            "khoan": khoan,
            "diem": diem,
            "phu_luc": phu_luc,
            "van_ban": van_ban,
        }

        # t·∫°o b·∫£n ghi chu·∫©n ho√°
        normalized = {
            "doc_id": meta.get("filename") or "",
            "source_sha1": meta.get("sha256") or "",  # API ƒë√¥i khi cung c·∫•p sha256; n·∫øu kh√¥ng c√≥ s·∫Ω g√°n ·ªü ngo√†i
            "element_id": el.get("id") or str(uuid.uuid4()),
            "type": el.get("type"),
            "text": text,
            "metadata": meta_enriched,
        }
        out.append(normalized)
    return out

def attach_file_level_metadata(
    normalized: List[Dict[str, Any]],
    file_path: Path,
    source_sha1: str,
) -> List[Dict[str, Any]]:
    for row in normalized:
        row["doc_id"] = file_path.name
        if not row.get("source_sha1"):
            row["source_sha1"] = source_sha1
        # ƒë·∫£m b·∫£o c√≥ page_number n·∫øu API ƒë√£ ch√®n PageBreak
        # Unstructured s·∫Ω t·∫°o element type="PageBreak" v·ªõi metadata.page_number tƒÉng d·∫ßn n·∫øu b·∫≠t include_page_breaks
        # (kh·∫£ d·ª•ng cho m·ªôt s·ªë ƒë·ªãnh d·∫°ng; v·ªõi .docx c√≥ th·ªÉ kh√¥ng lu√¥n c√≥ trang)
    return normalized

def write_jsonl(records: Iterable[Dict[str, Any]], out_path: Path) -> int:
    out_path.parent.mkdir(parents=True, exist_ok=True)
    n = 0
    with out_path.open("w", encoding="utf-8") as f:
        for rec in records:
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")
            n += 1
    return n

# =====================
# Pipeline x·ª≠ l√Ω th∆∞ m·ª•c
# =====================

def preprocess_word_folder(
    inputs: List[Path],
    out_jsonl: Path,
    languages: List[str] = ["vie","eng"],
) -> int:
    """
    X·ª≠ l√Ω danh s√°ch file .doc/.docx ‚Üí out_jsonl (append).
    """
    total = 0
    out_jsonl.parent.mkdir(parents=True, exist_ok=True)

    for p in inputs:
        if not p.exists():
            print(f"‚ö†Ô∏è  B·ªè qua (kh√¥ng t·ªìn t·∫°i): {p}")
            continue
        if p.suffix.lower() not in (".doc", ".docx"):
            print(f"‚ö†Ô∏è  B·ªè qua (kh√¥ng ph·∫£i Word): {p.name}")
            continue

        print(f"üîπ Partition: {p.name}")
        elements = partition_word_via_unstructured_api(
            p,
            languages=languages,
            include_page_breaks=True,
            coordinates=True,
            encoding="utf-8",
        )
        source_sha1 = sha1_of_file(p)
        normalized = enrich_hierarchy(elements)
        normalized = attach_file_level_metadata(normalized, p, source_sha1)

        with out_jsonl.open("a", encoding="utf-8") as f:
            for rec in normalized:
                f.write(json.dumps(rec, ensure_ascii=False) + "\n")
                total += 1

    print(f"‚úÖ ƒê√£ ghi {total} element ‚Üí {out_jsonl}")
    return total

# ============
# V√≠ d·ª• ch·∫°y th·ª≠
# ============

if __name__ == "__main__":
    # Thay ƒë·ªïi ƒë∆∞·ªùng d·∫´n theo m√¥i tr∆∞·ªùng c·ªßa b·∫°n
    # (C√°c file m·∫´u user ƒë√£ upload ·ªü /mnt/data)
    sample_paths = [
        Path("dataset/LuaÃ£ÃÇt PCCC 2024 55_2024_QH15_621347.doc"),
        Path("dataset/NghiÃ£ ƒëiÃ£nh 105-2025-Nƒê-CP ngaÃÄy 15-05-2025 huÃõoÃõÃÅng daÃÇÃÉn LuaÃ£ÃÇt PhoÃÄng chaÃÅy, chuÃõÃÉa chaÃÅy vaÃÄ cuÃõÃÅu naÃ£n, cuÃõÃÅu hoÃ£ÃÇ.doc"),
        Path("dataset/QCVN 03 2023 BCA veÃÇÃÄ PhuÃõoÃõng tieÃ£ÃÇn phoÃÄng chaÃÅy vaÃÄ chuÃõÃÉa chaÃÅy_VN.doc"),
        Path("dataset/B.1. BaÃâng ƒëoÃÇÃÅi chieÃÇÃÅu quy hoaÃ£ch.docx"),
    ]
    out = Path("./pccc_word_elements.jsonl")
    preprocess_word_folder(sample_paths, out)
